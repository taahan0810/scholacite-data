Knowledge Probing Language models are shown to encode a wide variety of knowledge after being pretrained on a large-scale corpus. Recent studies probe PLMs for linguistic knowledge ([1]; [2]), world knowledge ([3]; [4]; [5]), actionable knowledge ([6]), etc. via methods such as cloze prompts ([7]; [8]) and linear classifiers ([9]; [10]). Although having explored extensive knowledge within PLMs, previous knowledge probing works have not studied ontological knowledge systematically. We cut through this gap to investigate how well PLMs know about ontological knowledge and the meaning behind the surface form.

Knowledge Reasoning Reasoning is the process of drawing new conclusions through the use of existing knowledge and rules. Progress has been reported in using PLMs to perform reasoning tasks, including arithmetic ([11]; [12]), commonsense ([13], [14]; [12]), logical ([15]) and symbolic reasoning ([12]). These abilities can be unlocked by finetuning a classifier on downstream datasets ([14]) or using proper prompting strategies (e.g., chain of thought (CoT) prompting ([12]) and generated knowledge prompting ([16])). This suggests that despite their insensitivity to negation ([17]; [18]) and over-sensitivity to lexicon cues like priming words ([19]; [20]), PLMs have the potential to make inferences over implicit knowledge and explicit natural language statements. In this work, we investigate the ability of PLMs to perform logical reasoning with implicit ontological knowledge to examine whether they understand the semantics beyond memorization.
