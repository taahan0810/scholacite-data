In exploring the question of whether Pretrained Language Models (PLMs) possess and understand ontological knowledge, several groups of scholarly works have been instrumental in framing our investigation.

To begin with, a body of research has delved into the capability of PLMs in understanding lexical semantics. This is exemplified by the work of [1], which explores the types of knowledge encoded by PLMs, including lexical type-level knowledge. This is instrumental in comparing their capability in understanding lexical semantics to their grasp of ontological knowledge. Additionally, [2] further contributes to this understanding by proposing a method for evaluating the presence of syntax trees in a neural network's word representation space, thereby highlighting the syntactic knowledge in PLMs. Moreover, [7] investigates how semantic attributes and their values are captured by PTLMs, adding to the discourse on PLMs' comprehension of semantic attributes, which is relevant to assessing their knowledge of ontology.

In parallel, there is a discussion on the role of PLMs as knowledge bases. The work of [3] significantly supports this concept by analyzing the relational knowledge present in a range of state-of-the-art pretrained language models. [4] enhances this discussion by exploring methods to accurately estimate the knowledge contained in language models. Moreover, [5] offers a review of how deep contextual language models can flexibly express relational knowledge, an aspect central to understanding the depth of ontological knowledge stored within PLMs.

The reasoning capabilities of PLMs are another central theme, with [6] examining large language models' ability to decompose high-level tasks into actionable steps. This concept of reasoning is further developed by [11], which proposes a new decoding strategy to improve the reasoning capabilities of PLMs. The paper by [12] aligns closely with our work by showing how chain-of-thought prompting can enhance reasoning in large language models. [14] demonstrates that PLMs can be trained to systematically reason over implicit knowledge, and [15] provides a comprehensive evaluation of logical reasoning tasks performed by these models. [19] rounds out this group with an overview of transformers' reasoning performance on various tasks, revealing the depth of their reasoning capabilities.

The methods used for probing PLMs are also a focal point, with [9] presenting an approach for designing and interpreting probe results, critical to our understanding of ontological knowledge in PLMs. [10] offers an alternative with information-theoretic probing for linguistic structure, and [18] suggests new probing tasks for analyzing the factual knowledge stored within PLMs.

Additionally, the commonsense knowledge capabilities of PLMs are explored, providing a comparative perspective. For instance, [13] examines the reliance of PLMs on commonsense knowledge for question answering, while [16] shows that external knowledge can improve commonsense reasoning. [17] highlights the limitations of PLMs in linguistic capacities, offering insights into the potential limitations in understanding ontological knowledge. [20] investigates PLMs' use of lexical cues, which is relevant to our inquiry into ontological knowledge processing.

Lastly, the factual prediction capabilities of PLMs are discussed, with works like [8] exploring how language models store and retrieve factual knowledge for question answering. This provides a benchmark for our work as we explore PLMs' abilities to predict based on ontological knowledge.

In synthesizing these contributions, we aim to construct a nuanced understanding of how PLMs know and understand ontological knowledge, and how this might be enhanced in future research.
