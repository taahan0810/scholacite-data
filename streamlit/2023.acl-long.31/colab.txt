Grounding in Vision-Language Models
Recent advancements in vision-language models (VLMs) have shown remarkable progress in grounding language to the physical world. Grounding, the ability to link language units to their referents in the environment, is fundamental for understanding and acquiring the meanings of words. Studies have demonstrated a variety of approaches to enhance grounding in VLMs, focusing on both theoretical frameworks and computational modeling. The exploration of cross-modal mapping and lexical acquisition through theoretical lenses has laid the groundwork for understanding the complexities of language grounding [1], [2]. These theoretical insights have been pivotal in identifying the challenges associated with grounding, such as the need for flexible acquisition strategies that can adapt to growing vocabularies from perceptual inputs.

On the computational front, models have been developed to learn static word-meaning mappings, formalizing the acquisition of language in a computationally tangible manner [3]. While these models offer a structured approach to grounding, their reliance on fixed mappings limits their flexibility and adaptability to new lexical items. This limitation is addressed by approaches that leverage associations [5], [6], [7], multimodal fusion [8], [9], [10], and referential grounding models [17], [20], [22], [23], [24], which advance our understanding of language learning by incorporating dynamic and context-aware strategies. Despite these advancements, the reliance on predefined lexical representations and the challenges of dataset constraints highlight the ongoing need for models that can generalize and adapt more effectively.

Recent work has aimed to bridge these gaps by utilizing visual inputs, such as video [11], [12] and imagery [13], [14], [15], to ground language. [25] used the learning of visually grounded representations. Additionally, research extends to models incorporating visual information for object recognition [26], learning from unlabeled web data [27], meta-learning frameworks [28], and visually-supervised language models [29]. These approaches, while innovative, often focus on symbols already known to the system, overlooking the potential for expanding the vocabulary with novel terms. Augmented natural language understanding models that incorporate visual information [30], [31] complement the visually-grounded approach of this work. Tasks that simulate the continual acquisition of language from visual inputs [40] resonate with the ongoing exploration of visually-grounded language learning. Zero-shot learning approaches using textual descriptions [43] and research on recognizing 'unseen' objects [48] align with the goals of learning new words without explicit grounding supervision.

The evolution of vision-language models, underscored by the development of open-source toolkits like Stanza [16] for text analysis, marks a significant advance in bridging the gap between textual and visual information. Through comprehensive reviews [32], the field benefits from a rich context, guiding the exploration towards minimalist [33] and universal representations [34] that aim for broader applicability and simplicity. Innovations extend to multi-modal detection [36], grounded [37], and region-based pretraining [38], showcasing the diversification of approaches to enhance model performance and adaptability. Fine-grained pre-training [39] further exemplifies the sector's growth, targeting precision in multimodal alignment.

Open-vocabulary and zero-shot object detection research intersect with the principles of open-world language learning, presenting challenges and advancements that inform the current study. Methods for object retrieval based on natural language queries [18], [19], zero-shot learning models [41], [42], and approaches to visual-semantic embedding [44] expand the capabilities of object detection without direct supervision. The development of zero-shot object detection frameworks [45], [46], [47], open-vocabulary detection [49], [50], and techniques for learning prompt representations [51] exemplify the strides taken towards more generalizable detection systems.

However, despite these advancements, pretrained vision-language models encounter limitations due to fixed output vocabularies, which restrict their capability to generalize across new utterances and adapt to evolving linguistic expressions. This highlights the ongoing challenge of developing models that not only excel in multimodal integration but also remain flexible and responsive to the dynamic landscape of language use.

The incorporation of computational frameworks for generating object descriptions and resolving referring expressions has partially addressed these challenges [17], [20], [22], [23], [24]. However, issues such as ambiguity and the difficulty of generalization, compounded by dataset limitations, underscore the need for further innovation in models that can adapt and generalize across diverse linguistic and perceptual contexts.

Our work introduces Grounded Open Vocabulary Acquisition (GOVA) and the World-to-Words (W2W) model, which builds upon the foundation laid by previous studies. By focusing on the acquisition of grounding as a primary objective during pre-training, W2W aims to address the limitations of existing models. It leverages fine-grained word-object mappings to enable more effective learning of grounded word meanings, demonstrating the potential for fast mapping of novel terms in a few-shot learning context. This approach not only advances the field of grounded language acquisition but also offers new directions for future research in creating more adaptable and flexible vision-language models.
