Vision-Language Mapping. Mapping plays a central role in classic lexicon acquisition problem ([1]; [2]). Primarily, researchers focused on grounding words to their meaning symbols, building learning mechanisms using specific mental biases to simulate children’s word acquisition, and giving computational accounts for psycholinguistic phenomena ([3]; [4]; [5]; [6]). Early efforts along this line incorporate visual grounding either by learning a statistical or neural mapping from object categories ([7]; [8]; [9]; [10]; [11]) and more complicated visual features ([12]; [13]; [15]) to linguistic labels. These studies are usually in a closed world with limited vocabulary ([17]), and words are usually isolated from the natural context of use. More recently, multi-modal understanding tasks, e.g., object retrieval ([18]; [19]), referring expression comprehension and grounding ([20]; [21]; [22]; [23]), and phrase grounding ([24]) map referring expressions to corresponding objects. Our setup is closely related to this line as we position grounding as an explicit word-referent mapping problem. The difference is that, our work goes beyond grounding to study open-vocabulary acquisition through fast mapping, a more complicated but realistic challenge faced by AI agents.

Vision-Language Pre-training. Distributional word representations can be acquired through language modeling, and developing language models from visual data has been extensively studied by the community ([25]; [26]; [27]; [28]). Recent years have seen increasing research to enrich language representations with visually-augmented language modeling ([29]; [30]; [31]) and to learn multimodal representations with vision-language pre-training (VLP) ([32]). We are particularly interested in VLP models with fine-grained grounding objectives, e.g., Word-Region Alignment (WRA). These models either pre-train with weakly supervised alignment algorithms like optimal transport that matches words with patches ([33]) or proposals from a frozen detector ([34]; [35]), or perform explicit word grounding by pre-training a language-conditioned detector ([36]; [37]; [38]; [39]). Our model falls along this line, which jointly performs language modeling, object localization, and grounding during pre-training, rather than relying upon a pre-existing object detector.

Vision-Language Tasks. To evaluate vision-language systems, many downstream tasks have been formulated. Some related formulations are summarized in Table 5 in Appendix. While demonstrating some vision-language capabilities, these down-stream tasks provide limited insights into whether these models truly capture the grounded meaning of words with respect to the external environment. Our task design specifically targets the machine’s ability to predict words and ground words to perception. More akin to our formulation is the vision-based language modeling task ([40]) in a continual learning setting. Our work differs mainly in two aspects. First, the task proposed by [40] only predicts masked tokens based on the visual context, which leaves the referential uncertainty (i.e., grounding) unattended (e.g., in Figure 2, correct prediction of the word “boat” does not guarantee correct grounding). Also, this work primarily focuses on compositionality, while we seek to address few-shot grounded word learning when unseen words are encountered after pre-training.

Open-Vocabulary Object Detection. Early works formulate fast mapping of new words as a zero-shot object classification problem, which aims to generalize from known object labels to unknown ones ([41]; [42]; [43]; [44]). The setting later extends to a localization task, referred to as zero-shot object detection (ZSD) ([45]; [46], [47]; [48]). More recently, open-vocabulary object detection (OVD) ([49]; [50]; [51]; [52]) combines ZSD with weakly supervised object detection (WSD) to address the unrealistic constrain of traditional zero-shot settings. OVD assumes the availability of coarse-grained image-caption pairs, and attempts to generalize from limited fine-grained annotation of object categories to unseen ones. Nevertheless, this line of work positions words as object categories and isolates them from their linguistic context (e.g., sentences). Our setup instead challenges models to perform language modeling in human-generated captions.
