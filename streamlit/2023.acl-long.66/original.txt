<RESEARCH-QUESTION><BACKGROUND>While not related to code-switching, there has been prior work on alternatives or modifications to pretraining objectives like MLM. [1] is one of the first works to identify the lack of linguistically intuitive pretraining objectives. They propose new pretraining objectives which perform similarly to MLM given a similar pretrain duration. In contrast, [2] sticks to the standard MLM objective, but questions whether masking only 15% of tokens in a sequence is sufficient to learn meaningful representations. [3] maintains that higher masking up to even 80% can preserve model performance on downstream tasks. <RESEARCH-GAP>All of the aforementioned methods are static and do not exploit a partially trained model to devise better masking strategies on the fly. <BACKGROUND>[4] suggests time-invariant masking strategies which adaptively tune the masking ratio and content in different training stages. <EXTENSION>Ours is the first work to offer both MLM modifications and architectural changes aimed specifically at code-switched pretraining.

<APPROACH><BACKGROUND>Prior work on improving code-switched NLP has focused on generative models of code-switched text to use as augmentation ([5]; [6]; [7]), merging real and synthetic code-switched text for pretraining ([8]; [9]), intermediate task pretraining including MLM-style objectives ([10]). <RESEARCH-GAP>However, no prior work has provided an in-depth investigation into how pretraining using code-switched text can be altered to encode information about language transitions within a code-switched sentence. <COMPARE-CONTRAST>We show that switch-point information is more accurately preserved in models pretrained with our proposed techniques and this eventually leads to improved performance on code-switched downstream tasks.
