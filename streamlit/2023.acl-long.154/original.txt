Low-resource language translation: Despite very successful recent advances in neural machine translation, most of the gains have only benefited a handful of so called high-resource languages, which have enough textual resources to satisfy the substantial data requirements of state-of-the-art techniques. The vast majority of the world’s languages are low-resource, and researchers have increasingly been focusing on evaluating performance in this challenging setting [1].

Benchmarks: Traditionally, one of the biggest challenges to the development of low-resource translation systems has been the lack of high quality evaluation data. Several benchmarks focus on specific sets of languages, such as the MADAR dataset for Arabic dialects ([2]), the Autshumato benchmark covering 11 South African languages ([3]), or the TICO-19 benchmark covering 35 languages for the domain of medical information related to the COVID-19 pandemic ([4]). More recently, the FLORES-101 dataset ([5]) and its expansion to over 200 languages ([6]) has enabled multilingual evaluation across tens of thousands of directions, including many low-resource languages. Its domain is composed of an even mixture of travel guides (Wikitravel), children’s literature (Wikijunior), and news content (Wikinews).

Training corpora: Much important work has gone towards the development of parallel corpora for low-resource languages, most of which focusing on individual language pairs ([7]; [8]; [9]; [10], inter alia). [11] study the case of 15 low-resource African languages, most of which already have tens or hundreds of thousands of parallel sentences in the religious domain, and investigate how combining pre-trained models and a newly created corpus can lead to effective domain transfer.

Low-resource training: Amongst the techniques that can be used to decrease the reliance on manually annotated data, bitext mining ([12]; [13]) enables finding pairs of translations among large collections of unannotated monolingual text. [14] show its effectiveness for low-resource languages, but point out that it can be limited for the most data scarce languages. Backtranslation ([15]; [16]) can be used to create pseudo-parallel data from monolingual data in a target language. It relies on an initial, potentially low-quality translation model – thereby having some requirements on annotated data – and can also be applied iteratively for improved performance ([17]). Self-supervision ([18]) is a method employing monolingual text denoising as a joint training objective, and its use has been suggested as a way of kick-starting an iterative backtranslation pipeline. Finally, multilingual translation, which is often combined with one or more of the above techniques, has been shown to improve low-resource translation performance via cross-lingual transfer ([19]; [20]; [21]; [22]; [23]; [6]).

Training without parallel data: Within the area of low-resource translation, [24] describe the development of translation systems for low-resource languages without using any parallel data at all, relying instead on crawled monolingual data and language transfer. Methods which don’t require parallel data are likely complementary to the seed data approach proposed in this paper. However, the over-reliance on cross-lingual transfer from a high-resource language opens up the risk of a translation system flattening the differences between related languages, as observed by [6] for Arabic dialects. This is a particularly thorny issue for communities of speakers of endangered languages which are at risk of being displaced by a related higher-resource language – as is the case for several of the languages covered in this paper. In such cases, we recommend the seed data approach, which opens the door for the communities to take ownership in preserving their languages, and aligns well with their desire to preserve the distinctiveness of their language in technological applications.

Crisis MT: Low-resource machine translation has been studied in the context of crisis events, and has been proposed as a component of a rapid response infrastructure ([25]). In particular, [26] describe the creation of a system for Haitian Creole after the devastating 2010 earthquake, and [4] built a dataset to facilitate access to information related to the COVID-19 pandemic.
