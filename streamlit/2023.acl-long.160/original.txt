Multiple data-to-text and table-to-text tasks have been presented in the literature, such as WebNLG ([1]; [2]; [3]), DART ([4]), ToTTo ([5]), and WikiTableT ([6]), which primarily consist of data from general-purpose sources like Wikipedia. Several large language models ([7]; [8]; [9]) have been trained on large scale table-to-text corpora ([10]) to perform fact verification. However, these models may not perform well on specific domains they have not been trained on, such as e-commerce ([11]; [12]). Therefore, we must either find a way to easily generate new data to train large data-to-text models, or use unsupervised methods. Recently, [13] attempted to augment training data using GPT-3 ([14]), and [15] employed an information retrieval system to build prototypes for the generation. Our work makes orthogonal contributions to these studies, as we directly utilize the underlying unpaired data and text of a target corpus without recourse to any additional information retrieval or generation systems. Further, the above-mentioned data-to-text tasks have been evaluated primarily on automatic word- or n-gram-level metrics such as BLEU ([16]) or METEOR ([17]), with minimal (and mostly subjective) evaluation of faithfulness. In this work, we design a novel annotation schema to perform a more comprehensive evaluation of the faithfulness of the generated text to the input data.

Cycle training ([18]; [19]) relies on two models which are essentially inverse transforms of each other that are used to create “cycles”, which should return identical output to the input given. There are two distinct forms of cycle training. The first form ([19]) aims to learn to transform from one input form to another, e.g., to learn rotations of a car in one image to another. The second is the use of a “cycle consistency loss” as an auxiliary loss to some other task, e.g., in generative adversarial networks performing style transfer on images ([18]). NLG typically relies on models which are auto-regressive and non-differentiable. This precludes the direct use of cycle consistency losses ([20]; [21]; [22]). Nonetheless, we can still use cycle training via an alternating training strategy where we freeze one model and train the other, and vice versa ([23]; [21]). In this work, we train solely using cycle consistency. Cycle training has been recently applied to language processing tasks. In one text-to-text application, [24] use a similar unsupervised methodology to perform bidirectional text transformations for converting keyword search queries to natural language questions, and vice versa. It has also been used for Named Entity Recognition in the absence of large annotated text ([22]). In this case, one model extracts entities, and the inverse model creates text from those entities. The approach is limited by the fact that there are many ways to realize sentences with the same entities. Put differently, there is no strong requirement of cycle consistency, and this will become even more apparent as we analyze the conditions under which cycle training works well in data-to-text tasks.

To the best of our knowledge, the only work to explicitly call out the self-consistency requirement of data-to-text generation tasks is the CycleGT model ([20]) developed for data-to-text generation on the WebNLG dataset. One key advantage of cycle training is that it need not rely on any supervision, and instead relies primarily or solely on the self-consistency of inputs and outputs. However, CycleGT relies on a pre-existing NER model to extract entities from the output text. The authors then train an inverse model to predict the links between entities and predicates. Should the entities not be recognized by their NER system, the model will fail overall; this is not an uncommon situation in applications such as online shopping ([11]; [25]), where entities are complex or change frequently ([26]). In principle, a separate NER model could be built using cycle training, as in CycleNER ([22]), but the CycleGT authors did not do so. In this work, we design a simple approach using pre-trained language generation models, fine-tuned for both data-to-text and text-to-data generation cycles.
