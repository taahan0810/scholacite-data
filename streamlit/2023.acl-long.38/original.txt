Lexicalized neural models. Word-level alignments between input and output sequences were an essential feature of statistical phrase- and tree-based sequence models ([1]; [2]). Neural scoring functions were sometimes integrated into these models ([3]). Neural models with attention ([4]) do not require explicit alignment, though several pieces of past work have shown that incorporating explicit token-level correspondences improves generalization ([5]; [6]; [7]). The semantic correspondence function in Sec. 4 plays the same role as the input–output dictionary in these methods, but LEXSYM as a whole is more general: it is not restricted to modeling sequence-to-sequence problems, and can infer and exploit correspondence relations between component of an example. To the best of our knowledge, this paper is also the first to make use of token-level alignments in joint neural models of text and images.

Compositionality in representation learning. While we have focused on compositionality as a property of data distributions or interpretation functions, another line of work in machine learning and language evolution has studied compositionality as an emergent property of learned representations ([8]; [9]; [10]). In settings where representational compositionality is desirable (e.g. to train communication protocols that can generalize to new states), LEXSYM might provide a tool for promoting it.

Equivariant Sequence Models. As mentioned in Sec. 2, our work builds on existing approaches that control generalization with specialized model architectures designed to be equivariant to permutations of a pre-specified lexicon (if f (x1 · · · xn ) = y1 · · · ym then f (π(x1 ) · · · π(xn )) = π(y1 ) · · · π(ym ) for a permutation π) ([11]; [12]). LEXSYM differs from these approaches in three ways. First, LEXSYM is model-agnostic and compatible with pre-training. Second, LEXSYM is compatible with (and automatically derives transformations for) more complicated relations than input–output correspondences, making it possible to apply to tasks like ALCHEMY where such relations are important. Finally, LEXSYM gracefully handles (possibly noisy) learned lexicons, making it applicable to tasks like COGENT with complex or uninterpretable token mappings.

Data Augmentation. Data augmentation approaches are widely used across machine learning application domains featuring known invariances of the data distribution ([13]; [14]; [15]). Substitution-based schemes that replace words with synonyms, or synchronously replace words and their translations, are widely used for machine translation and general de-biasing ([16]; [17]; [18]).
