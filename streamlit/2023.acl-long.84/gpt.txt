Stereotypes and Bias in Language Models. Language models have become a focal point for current research due to their deep-rooted ability to learn and reproduce human-like biases. As evidenced by [11], even semantically derived language corpora can manifest such biases. Recognizing the risks posed by language models, including the perpetuation of stereotypes, [13] outlines the ethical and social implications, which resonate with our study's emphasis on stereotype identification and mitigation. The prevalence of gender bias in text, as discussed by [14], further underscores the urgency of addressing biases within language models, a matter which our work-in-progress investigates. The introduction of benchmarks such as the CrowS-Pairs [15] and Stereoset [16] contributes to the measurement of social bias, guiding our endeavor to quantify these biases. Furthermore, methods proposed by [17] and [18] for identifying and reducing bias in language models underpin our methodology. Intersectional biases, as explored in [19] and [20], provide additional layers of complexity that our work addresses. Crucially, the automatic social bias discovery method described in [21] aligns with our framework's objective to measure and understand biases in language models. Papers like [22], [30], and [31] offer theoretical and practical tools for examining stereotypes within language models, directly feeding into our analysis of stereotypes and biases in LLM outputs.

Markedness and Social Constructs. The theoretical landscape of markedness is essential in understanding social and cultural constructs, particularly as they apply to our work-in-progress which uses the concept of marked personas. Levi-Strauss's [1] seminal work on structural anthropology, though not explicitly detailed here, provides a foundational backdrop for our approach. The concept of markedness has been a cornerstone in linguistics, as detailed by [2], allowing us to frame our study within this established semantic structure. The sociology of the unmarked, as discussed by [8], offers a lens to view the often overlooked default cultural biases, which our research seeks to bring to light in the context of language models.

Gender and Intersectionality. The intersectionality of gender and other demographic dimensions is central to our understanding of bias in language models. Beauvoir's [3] foundational text on gender studies provides historical context, shaping our appreciation of gender bias. Collins's [6] exploration of the power of Black women as knowledge agents is instrumental in informing our intersectional analysis. Hooks's [7] criticism of white feminism deepens our understanding of the complexities within feminist movements and the need for a nuanced approach. The analysis of gender bias within organizations in [10] parallels our examination of stereotypes and professional advancement. The Combahee River Collective's [25] discussion on identity politics adds a valuable perspective on the power of identities, while [26], [28], and [29] provide comprehensive analyses of intersectional stereotypes and methodologies for detecting biases in language models, enriching our own intersectional study.

Ethnic Stereotypes and Changes Over Time. Understanding how ethnic stereotypes have evolved is crucial for our research. The Princeton trilogy revisited and revised [23] offers a longitudinal perspective on changes in ethnic and national stereotypes, relevant to our historical context. Similarly, the cross-temporal meta-analysis [24] shows how gender stereotypes have changed over the decades, affirming the dynamic nature of stereotypical portrayals which our study considers.

Cultural Perspectives and Bias. Cultural biases are multifaceted and shape perceptions in numerous ways. While [4] could not provide an explicit rationale for inclusion, its focus on colonial impacts on bias is suggestive. The notion of 'masculine defaults' [5] and how they affect representation is directly related to cultural biases our work aims to address. The critical evaluation of benchmark datasets for stereotyping in NLP tasks [12] challenges the validity of these models and supports our approach that seeks to understand and mitigate biases through more contextualized and nuanced means.
