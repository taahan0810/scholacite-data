Stereotypes and Bias in Language Models. Language models have garnered significant attention for their capacity to learn and mirror human biases, an issue highlighted by the finding that semantically derived language corpora inherently contain biases [11]. The ethical and social risks associated with language models, including the reinforcement of stereotypes, are critically examined [13], echoing the concerns our study aims to address through the identification and mitigation of such stereotypes. The critical issue of gender bias in text [14], alongside the innovative benchmarks CrowS-Pairs [15] and StereoSet [16] for measuring social bias, guides our efforts to quantify these biases. Our methodology is supported by the proposals for bias identification and reduction in language models [17], [18], with a special focus on intersectional biases [19], [20] and the pursuit of automatic social bias discovery [21]. Theoretical and empirical frameworks for analyzing stereotypes within language models [22], [30], [31] are integral to our analysis of LLM outputs.

Markedness and Social Constructs. The theoretical framework of markedness is pivotal for understanding social and cultural constructs, particularly in the context of our work, which utilizes the concept of marked personas. The foundational work in structural anthropology by Levi-Strauss [1] and the seminal discussions on markedness [2] provide a backdrop for our approach. The sociology of the unmarked [8] further illuminates default cultural biases, a focal point of our research in the context of language models.

Intersectionality, Gender, and Ethnic Stereotypes. The relationship betweeen gender with other demographic facets is crucial to our understanding of bias in language models. The historic insights from Beauvoir [3] on gender, Collins's work on the empowerment of Black women [6], and Hooks's critique of mainstream feminism [7] inform our intersectional analysis. Discussions on gender bias within organizations [10], identity politics by the Combahee River Collective [25], and comprehensive analyses of intersectional stereotypes and bias detection methods [26], [28], [29] enrich our study, providing a multidimensional perspective on biases in LLMs. Furthermore, the evolution of ethnic stereotypes, revisited through the Princeton trilogy [23] and a cross-temporal analysis [24], offers a longitudinal view essential for understanding the dynamic nature of stereotypes, a consideration pivotal to our research.

Cultural Perspectives and Bias. The multifaceted nature of cultural biases, including the impact of colonialism [4] and the concept of masculine defaults [5], shape perceptions and representations significantly. The critique of benchmark datasets in NLP for perpetuating stereotypes [12] underscores the importance of a nuanced approach to understanding and mitigating biases, aligning with our objective to explore these issues through more contextualized means. Our work also considers the broader implications of biases within language models, including misinformation [13], the challenges in gender bias classification [14], and the importance of theoretical and measurement tools in understanding social stereotypes [22]. The exploration of pollution as a form of colonialism [4], the significance of mitigating hidden cultural biases [5], and the evaluation of benchmark datasets' validity [12] further complement our comprehensive approach to studying biases in language models.
