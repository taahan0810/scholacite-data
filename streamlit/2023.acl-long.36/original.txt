Several works have worked on measuring the performance of MT models on contextual discourse phenomena. The first example of this was done by [1], which evaluated automatically the precision and recall of pronoun translation in statistical MT systems. [2] proposed evaluating models on pronoun translation based on a pairwise comparison between translations that were generated with and without context, and later [3] extended this work to include more languages and phenomena in their automatic evaluation/test set creation. These works rely on prior domain knowledge and intuition to identify context-aware phenomena, whereas we take a systematic, data-driven approach.

Most works have focused on evaluating performance in discourse phenomena through the use of contrastive datasets. [4] automatically create a dataset for anaphoric pronoun resolution to evaluate MT models in EN → DE. [5] manually creates a dataset for both pronoun resolution and lexical choice in EN → FR. [6][7] creates a dataset for anaphora resolution, deixis, ellipsis and lexical cohesion in EN → RU. However, [8] suggest that translating and disambiguating between two contrastive choices are inherently different, motivating our approach in measuring direct translation performance.
