Training multilingual LLMs using the masked language modeling (MLM) objective is effective to achieve cross-lingual representations ([1]; [2]). These models can be further improved by incorporating techniques such as discriminative pre-training ([3]) and the use of parallel data ([4]; [5]). However, this primarily benefits a limited set of languages with large corpora.

Recent research has attempted to extend existing LLMs to languages with limited resources. [6] propose vocabulary extension; [7] investigate adaptation methods, including MLM and Translation Language Model (TLM) objectives and adapters; [8] adapt XLM-R to 17 African languages; [9] expand language models to low-resource languages using bilingual lexicons.

Alternatively, parameter-efficient fine-tuning adapts pre-trained models to new languages by training a small set of weights effectively ([10]; [11]; [12]). [13] address the “curse of multilinguality” by sharing a part of the model among all languages and having separate modules for each language. <COMPARE-CONTRAST>We show that the common perception that multilinguality increases as we add more languages, until, from some point, it starts decreasing, is naive. The amount of available data per language and the similarity between languages also play important roles (§6.8).

Another approach trains LLMs from scratch for a limited number of tail language; e.g., AfriBERTa ([14]) and IndicNLPSuite ([15]) are LLMs for 11 African languages and 11 Indic languages. <COMPARE-CONTRAST>In concurrent work, [16] train a multilingual model for 517 African languages on a 42 GB corpus, but without making the model available and with an evaluation on a smaller number of languages than ours.

Closely related to our work on corpus creation, [17] and [18] also create NLP resources for a large number of tail languages. They train a language identifier model and extract textual data for tail languages from large-scale web crawls. <RESEARCH-GAP>This approach is effective, but it requires significant computational resources and native speakers for all tail languages. This is hard to do outside of large corporations. [17] have not made their data available. [18] have only released a portion of their data in around 200 languages.

A key benefit of “horizontally” scaled multilingual LLMs is transfer from high- to low-resource languages. <USES>Our evaluation suggests that Glot500-m excels at this, but this is not the main focus of our paper. <BACKGROUND>There is a large body of work on crosslingual transfer: ([19]; [20]; [21]; [2]; [22]; [23]; [24]; [25]; [26]), inter alia.
