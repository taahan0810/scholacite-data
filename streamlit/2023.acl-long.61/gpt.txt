Multilingualism in language technology is a rapidly developing field, and several scholarly efforts have been undertaken to tackle the complexity of scaling multilingual LLMs. Seminal work in this area was introduced with BERT [1], which laid the foundation for pre-training deep bidirectional representations and significantly influenced the subsequent development of models like Glot500-m. Further advances were made by the introduction of XLM-R [2], which extended the capabilities of transformers in mastering cross-lingual representation learning, and played a pivotal role in understanding the performance of models on low-resource languages. Another significant contribution came from the introduction of ELECTRA-style tasks [3], which offered new strategies for enhancing cross-lingual transferability, a key goal for comprehensive language models. Alternating Language Modeling (ALM) [4] provided another methodology, employing code-switching to better capture linguistic nuances. Lastly, the InfoXLM framework [5] presented a contrastive learning-based pre-training task, demonstrating an information-theoretic approach to refine multilingual language models' cross-lingual transferability.

The development of techniques to bolster low-resource language models is another crucial aspect of multilingual NLP. A variety of methods have been proposed to improve support for languages with limited resources. Vocabulary expansion has been explored as a means to enhance multilingual pre-trained models [6], while other research has investigated adapting pre-trained multilingual models to new languages through resources like the New Testament [7]. Multilingual adaptive fine-tuning has been suggested to better tailor models to African languages [8], and lexicon-based adaptation has been posited as an innovative strategy to extend the reach of NLP technology to under-represented languages [9]. Moreover, efficient alternatives to fine-tuning, such as masking strategies [10], have been proposed, as well as novel approaches for adapting multilingual models to low-resource languages and unseen scripts [11]. Challenging the conventional focus on high-resource languages, some researchers have explored the viability of training a multilingual language model exclusively on low-resource languages [14].

A myriad of approaches have been ventured into for multilingual representation learning. Sparse fine-tuning methods capable of preventing catastrophic forgetting and enabling effective cross-lingual transfer have been presented [12], while the development of modular transformers [13] has shown the potential for positive transfer and enhanced performance. Massively multilingual sentence embeddings [19], few-shot language transfer with multilingual transformers [21], and the study of linguistic variation encoded by sentence encoders [25] all constitute different forays into the territory of multilingual representation learning. Additionally, the exploration of high-resource languages other than English for transfer learning has provided an alternative perspective on multilingual representation learning [22], and the development of Many-to-Many multilingual translation models [23] has pushed the boundaries beyond English-centric approaches.

Machine translation for low-resource languages has not been overlooked, with remarkable efforts being made to build translation systems supporting over one thousand languages [17]. The pursuit to scale human-centered machine translation to encompass a wide array of languages [18] and the extraction of named entity resources for a multitude of languages [24] have each highlighted the challenges and solutions unique to low-resource languages.

Finally, the consideration of language-specific pre-trained models has led to research on the creation and application of models tailored to specific language families or regions. For instance, resources and models for Indian languages [15] and massively multilingual models for African languages [16] have been developed. Graph-based label propagation methods for part-of-speech tagging [20] and systems designed for sentiment analysis in African languages [26] further underscore the importance of language-specific approaches in the landscape of multilingual language modeling.

In totality, these scholarly contributions provide a crucial backdrop to our work on Glot500, offering insightful techniques, methodologies, and findings that underpin our approach to scaling LLMs horizontally across a vast array of languages, including those traditionally underserved by NLP technologies.
